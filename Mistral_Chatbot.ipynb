{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravKrGuptaDS/Mistral_Conversational_Chatbot/blob/main/Mistral_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU."
      ],
      "metadata": {
        "id": "DNrZtnUyYheg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "aNTmMJIMYjiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "To load our desired model, `mistralai/Mistral-7B-Instruct-v0.2`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n",
        "1. Use the Hugging Face CLI to login and verify your authentication status.\n",
        "\n"
      ],
      "metadata": {
        "id": "BO2pb-EeZA95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "_6AfgF3_arYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "whoami()"
      ],
      "metadata": {
        "id": "ED0c-pSPRfbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model"
      ],
      "metadata": {
        "id": "xmJHSjx4abta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "jsBrtGpZYmcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Responses"
      ],
      "metadata": {
        "id": "cKHfNL76a6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation history\n",
        "conversation = []\n",
        "\n",
        "def build_prompt(conversation):\n",
        "    \"\"\"Convert conversation list into a prompt string.\"\"\"\n",
        "    prompt = \"\"\n",
        "    for turn in conversation:\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            prompt += f\"User: {turn['content']}\\n\"\n",
        "        else:\n",
        "            prompt += f\"Assistant: {turn['content']}\\n\"\n",
        "    prompt += \"Assistant:\"  # signal model to continue\n",
        "    return prompt\n",
        "\n",
        "def chat(user_input):\n",
        "    \"\"\"Add user input, generate assistant reply, and update history.\"\"\"\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "    prompt = build_prompt(conversation)\n",
        "\n",
        "    output = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Extract generated text\n",
        "    reply = output[0][\"generated_text\"].split(\"Assistant:\")[-1].strip()\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
        "    return reply\n",
        "\n",
        "user_input = [...]\n",
        "\n",
        "# Chat loop\n",
        "print(\"Chatbot ready! Type 'bye', 'quit', or 'exit' to stop.\\n\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Assistant: Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "    response = chat(user_input)\n",
        "    print(\"Assistant:\", response)\n"
      ],
      "metadata": {
        "id": "khLTFd6Gg5Ej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}